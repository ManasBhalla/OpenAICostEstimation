{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJoTRRTfza-_",
        "outputId": "5f0c6d13-74ca-478a-a2b3-398ce1bba101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n",
            "Collecting openai\n",
            "  Downloading openai-1.8.0-py3-none-any.whl (222 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typing-extensions, h11, httpcore, httpx, openai\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.8.0 typing-extensions-4.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade tiktoken\n",
        "%pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "model_encoding_dict={\n",
        "    'cl100k_base':\t['GPT-3.5-Turbo-4k', 'GPT-3.5-Turbo-16k', 'GPT-3.5-Turbo-1106', 'GPT-4-Turbo', 'GPT-4-Turbo-Vision', 'GPT-4-8k', 'GPT-4-32k', 'text-embedding-ada-002'],\n",
        "    'p50k_base'\t:['text-davinci-002', 'text-davinci-003'],\n",
        "}\n",
        "\n",
        "azure_costing= {\n",
        "    'Models': ['GPT-3.5-Turbo-4k', 'GPT-3.5-Turbo-16k', 'GPT-3.5-Turbo-1106', 'GPT-4-Turbo', 'GPT-4-Turbo-Vision', 'GPT-4-8k', 'GPT-4-32k', 'Babbage-002', 'Davinci-002'],\n",
        "    'Prompt': [0.0015, 0.003, 0, 0.01, 0.01, 0.03, 0.06, 0.0004, 0.002],\n",
        "    'Completion': [0.002, 0.004, 0, 0.03, 0.03, 0.06, 0.12, 0, 0],\n",
        "    'Usage': [0, 0, 0, 0, 0, 0, 0, 0.0004, 0.002]\n",
        "}\n",
        "\n",
        "azure_costing_df = pd.DataFrame(azure_costing)\n",
        "\n",
        "def tiktoken_setup():\n",
        "  encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "  encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "  \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "  encoding = tiktoken.get_encoding(encoding_name)\n",
        "  num_tokens = len(encoding.encode(string))\n",
        "  return num_tokens\n",
        "\n",
        "def map_model_encoding(model: str):\n",
        "  for key in model_encoding_dict:\n",
        "    if model in model_encoding_dict[key]:\n",
        "      encoding=key\n",
        "      print('Model found - using encoding model ', key)\n",
        "  return encoding\n",
        "\n",
        "def costing(model, input_tokens, output_tokens):\n",
        "  result = azure_costing_df[azure_costing_df['Models'] == model]\n",
        "  print(result)\n",
        "  if model=='Babbage-002' or model=='Davinci-002':\n",
        "    cost=result['Usage']*((input_tokens+output_tokens)/1000)\n",
        "  else:\n",
        "    cost=(result['Prompt'].values[0]*(input_tokens/1000))+(result['Completion'].values[0]*(output_tokens/1000))\n",
        "    print('prompt: ', result['Prompt'].values[0], 'tokens: ', input_tokens, 'completion: ', result['Completion'], 'output: ', output_tokens)\n",
        "\n",
        "  print(cost)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "  tiktoken_setup()\n",
        "  # model=input(\"What is the model you are using: \")\n",
        "  model='GPT-4-8k'\n",
        "  encoding=map_model_encoding(model)\n",
        "  print(encoding)\n",
        "  # sample_input=input(\"Please enter the last input used in your GPT model: \")\n",
        "  # sample_output=input(\"Please enter the output generated for the entered input: \")\n",
        "  sample_input=\"\"\"\n",
        "  turn the following data into a pandas df\n",
        "\n",
        "\n",
        "Language models\n",
        "Models \tContext \tPrompt (Per 1,000 tokens) \tCompletion (Per 1,000 tokens)\n",
        "GPT-3.5-Turbo \t4K \t$0.0015 \t$0.002\n",
        "GPT-3.5-Turbo \t16K \t$0.003 \t$0.004\n",
        "GPT-3.5-Turbo-1106 \t16K \tN/A \tN/A\n",
        "GPT-4-Turbo \t128K \t$0.01 \t$0.03\n",
        "GPT-4-Turbo-Vision \t128K \t$0.01 \t$0.03\n",
        "GPT-4 \t8K \t$0.03 \t$0.06\n",
        "GPT-4 \t32K \t$0.06 \t$0.12\n",
        "Base models\n",
        "Models \tUsage per 1,000 tokens\n",
        "Babbage-002 \t$0.0004\n",
        "Davinci-002 \t$0.002\n",
        "  \"\"\"\n",
        "  sample_output=\"\"\"\n",
        "  You can create a pandas DataFrame with the provided data using the following code:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Models': ['GPT-3.5-Turbo', 'GPT-3.5-Turbo', 'GPT-3.5-Turbo-1106', 'GPT-4-Turbo', 'GPT-4-Turbo-Vision', 'GPT-4', 'GPT-4', 'Babbage-002', 'Davinci-002'],\n",
        "    'Context': [4_000, 16_000, 16_000, 128_000, 128_000, 8_000, 32_000, None, None],\n",
        "    'Prompt (Per 1,000 tokens)': [0.0015, 0.003, None, 0.01, 0.01, 0.03, 0.06, 0.0004, 0.002],\n",
        "    'Completion (Per 1,000 tokens)': [0.002, 0.004, None, 0.03, 0.03, 0.06, 0.12, None, None],\n",
        "    'Usage per 1,000 tokens': [None, None, None, None, None, None, None, 0.0004, 0.002]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n",
        "```\n",
        "\n",
        "This will create a DataFrame with the provided data, and you can use it for further analysis or manipulation in your Python environment.\n",
        "  \"\"\"\n",
        "  input_tokens=num_tokens_from_string(sample_input, encoding)\n",
        "  output_tokens=num_tokens_from_string(sample_output, encoding)\n",
        "  costing(model, input_tokens, output_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "tVbXk73KzjyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUU_JyXO1yZa",
        "outputId": "e1707429-9665-4e40-a1d4-9ef803278343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model found - using encoding model  cl100k_base\n",
            "cl100k_base\n",
            "     Models  Prompt  Completion  Usage\n",
            "5  GPT-4-8k    0.03        0.06    0.0\n",
            "prompt:  0.03 tokens:  246 completion:  5    0.06\n",
            "Name: Completion, dtype: float64 output:  332\n",
            "0.027299999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLkGLe6w1RnS",
        "outputId": "9cdb21f7-7e6f-4d01-b9c9-e6b02e7a5a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}